######################################################################
# MAP estimates for Dawid and Skene Model with Priors
#
# This code extends the model and EM algorithm presented here:
#
#    A. P. Dawid and A. M. Skene. 1979.  Maximum Likelihood Estimation
#    of Observer Error-Rates Using the EM Algorithm.  Journal of Royal
#    Statistical Society, Series C (Applied Statistics) 28(1):20--28.
#    http://www.jstor.org/stable/2346806?origin=JSTOR-pdf
#
# as described in this paper:
#
#    Rebecca J. Passonneau and Bob Carpenter. 2014.  The Benefits of a
#    Model of Annotation.  Transactions of the Association for
#    Computational Linguistics 2(Oct):311âˆ’326.
#    http://www.transacl.org/wp-content/uploads/2014/10/taclpaper60.pdf
#
#
# source: https://github.com/bob-carpenter/anno/blob/master/R/em-dawid-skene.R
#
# USAGE in R:
#
# First change directories to the directory containing this file:
#
#     > setwd("anno/R");  
#
# then run the program:
#
#     > source("em-dawid-skene.R");
#
# You will need to change the hard-coded path in read.table and run
# it with data organized with a header and one row per label
# as (item ID annotator ID category ID) with a single tab separating
# each column.  All identifiers must be numbered sequentially from 1.
# The data from (Passonneau and Carpenter 2014) is included with the
# repo and provides an example.  Here's the head of add-v.tsv:
#
#    item       annotator       rating
#    1  1       6
#    2  1       3
#    3  1       1
#    4  1       3
#    5  1       1
#    6  1       1
#    ...
#
# Upon completion, you will find the following estimands set in
# the global namespace:
#
#   pi_hat[k]:          estimate of pi[k]
#   theta_hat[j,k,k']:  estimate of theta[j,k,k']
#   E_z[i,k]:           estimate of Pr[z[i]==k | data] 
#
# and the program writes three TSV files in the directory from which 
# it was run as output, for prevalence (pi), annotator accuracies
# and biases (theta), and predicted true category (z).
# 
#   pi_hat.tsv          estimate of pi
#   theta_hat.tsv       estimate of theta
#   z_hat.tsv           estimate of Pr[Z[i] == k | data]
#
######################################################################

# PROGRAM VARIABLES
# K: number of categories
# J: number of annotators
# I: number of annotations
# N: number of labels
# ii[N] in 1:I:  item for label n
# jj[N] in 1:J:  annotator for label n
# y[N] in 1:K:   category for label n

### READ DATA

# this next line assumes you read out of GitHub from the R directory
data <- read.table("./exdata/add-v.tsv", header=TRUE)
str(data)
print("finished reading data");

ii <- data$item;
jj <- data$annotator;
y <- data$rating;


N <- length(ii) # number of labels
K <- max(y) # number of categories
J <- length(unique(jj)) # number of annotators
I <- length(unique(ii)) # number of items

# print data sizes
print(paste("K=",K, "; N=",N, "; J=",J, ";I=",I));

##### EM ALGORITHM #####

### INITIALIZATION

# create a #(annotators) by #(categories) by #(categories) 'container' array, which
#  will evenutally containt the J K-by-K confusion matricees defining annotators
#  precision 
# in the case of binary classification, theta_j contains 
#  [
#   true positive rate, false positive rate;
#   true negative rate, false negative rate;
#  ]
theta_hat <- array(NA,c(J,K,K));

# for each annotator
j = 1
for (j in 1:J)
  # for each category
  k = 1
  for (k in 1:K) { # begin iteration over categeories 
    
    for (k2 in 1:K)
      theta_hat[j,k,k2] <- ifelse(k==k2, 0.7, 0.3/K);
    
    # estimator of of individual error-rates
    pi_hat <- array(1/K,K);
    
    ### EM ITERATIONS
    epoch <- 1;
    min_relative_diff <- 1E-8;
    last_log_posterior = - Inf;
    E_z <- array(1/K, c(I,K));
    MAX_EPOCHS <- 100;
    
    # 
    for (epoch in 1:MAX_EPOCHS) {
      ### E-step 
      for (i in 1:I)
        E_z[i,] <- pi_hat;
      for (n in 1:N)
        for (k in 1:K)
          E_z[ii[n],k] <- E_z[ii[n],k] * theta_hat[jj[n],k,y[n]];
        for (i in 1:I)
          E_z[i,] <- E_z[i,] / sum(E_z[i,]);
        
        ### M-step
        beta <- 0.01; 
        pi_hat <- rep(beta,K);          # add beta smoothing on pi_hat
        for (i in 1:I)
          pi_hat <- pi_hat + E_z[i,];
        pi_hat <- pi_hat / sum(pi_hat);
        
        alpha <- 0.01;
        count <- array(alpha,c(J,K,K)); # add alpha smoothing for theta_hat
        for (n in 1:N)
          for (k in 1:K)
            count[jj[n],k,y[n]] <- count[jj[n],k,y[n]] + E_z[ii[n],k];
          for (j in 1:J)
            for (k in 1:K)
              theta_hat[j,k,] <- count[j,k,] / sum(count[j,k,]);
            
            # create #(items)-by-#(cateories) array, which will contain
            #  items' class-membership probabilities 
            p <- array(0,c(I,K))
            
            # assign 
            for (i in 1:I) {
              p[i,] <- pi_hat
            }
            
            # loop over all annotations
            for (n in 1:N) { # begin iterating over annotations
              for (k in 1:K) {
                p[ii[n], k] <- p[ii[n], k] * theta_hat[jj[n], k, y[n]]
              }
              
              # initialize log posterior
              log_posterior <- 0.0
              
              # update log posteriro for each item
              for (i in 1:I) {
                log_posterior <- log_posterior + log(sum(p[i,]));
              }
              
              if (epoch == 1) {
                print(
                  paste("iteration nr. ", epoch,
                        ": log posterior = ", log_posterior, 
                        sep = "")
                )
              } else {
                diff <- log_posterior - last_log_posterior
                relative_diff <- abs(diff / last_log_posterior)
                print(
                  paste("iteration nr. ", epoch,
                        ": log posterior = ", log_posterior, 
                        ", relative difference = ", relative_diff,
                        sep = "")
                )
                if (relative_diff < min_relative_diff) {
                  print("Converged!")
                  break
                }
              }
              # update log posterior
              last_log_posterior <- log_posterior
            } # end iterating over annotations
    }
  } # end iteration over categeories 
    
    # VOTED PREVALENCE AS A SANITY CHECK; compare to estimates of pi
    voted_prevalence <- rep(0,K);
    for (k in 1:K)
      voted_prevalence[k] <- sum(y == k);
    voted_prevalence <- voted_prevalence / sum(voted_prevalence);
    print(paste("voted prevalence=",voted_prevalence));
    
    pi_out <- array(0,dim=c(K,2),dimnames=list(NULL,c("category","prob")));
    pos <- 1;
    for (k in 1:K) {
      pi_out[pos,] <- c(k,pi_hat[k]);
      pos <- pos + 1;
    }
    write.table(pi_out,sep='\t',row.names=FALSE,file="pi_hat.tsv",quote=FALSE);
    
    theta_out <- array(0,
                       dim=c(J*K*K,4),
                       dimnames=list(NULL,c("annotator","reference",
                                            "response","prob")));
    pos <- 1;
    for (j in 1:J) {
      for (ref in 1:K) {
        for (resp in 1:K) {
          theta_out[pos,] <- c(j,ref,resp,theta_hat[j,ref,resp]);
          pos <- pos + 1;
        }
      }
    }
    write.table(theta_out,
                sep='\t',
                row.names=FALSE,
                file="theta_hat.tsv",quote=FALSE);
    
    z_out <- array(0,dim=c(I*K,3),
                   dimnames=list(NULL,c("item","category","prob")));
    pos <- 1;
    for (i in 1:I) {
      for (k in 1:K) {
        z_out[pos,] = c(i,k,E_z[i,k]);
        pos <- pos + 1;
      }
    }
    write.table(z_out,sep='\t',row.names=FALSE,file="z_hat.tsv",quote=FALSE);

    
    
    
    
    
    
    
    
    
    
    
    
    






