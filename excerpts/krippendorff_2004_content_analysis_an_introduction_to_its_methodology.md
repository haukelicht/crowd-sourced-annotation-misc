krippendorff_2004_content_analysis_an_introduction_to_its_methodology.md
- Krippendorff (2004) *Content analysis: An introduction to its methodology.* 2nd edition 
    - **chapter 2: conceptual foundation** (pp. 18-25)
        - *definition* (pp. 18-21)  
            - content analysis (CA):= "a research technique [i.e., a specialized procedure] for making replicable and valid inferences from texts ... to the contexts of their use" (p. 18)
            - different approaches (p. 19)
                1. content *inherent* to text
                2. content as a *property of the source* of a text
                3. content as *emergent property of the process of analyzing/reading/interpreting* text relative to a particular 'context'
            - Krippendorff dismisses the first two approaches
                - (1.) is based on an 'implicit representationalsim' (p. 20): the employs an unwarranted container metaphor for the relation between text and meaning
                - (2.) conceives of text as a coding device, whose messages need only to be decoded
                - both "fail to acknowledge the analyst's own conceptual contributions to what constitute the appropriate reading of the analyzed texts and the relevance of this reading to a given research question" (p. 21)
                - he generally criticizes the measurement model underpinning popular conceptualizations of content analysis for it "implies that there is something inherent to text that is measurable without any interpretation by ... culturally competent analysts" (p. 22)
        - *espistemological elaborations* (pp. 21-5)
            - Krippendorf summarizes his approach to CA in six theses about the features of text (pp. 22-4):
                1. texts have no objective (reader-independent) qualities
                2. texts do not have single meanings that can be identified in/inferred from them
                3. the meanings invoked by texts need not be shared
                4. "Meanings (contents) speak to something other than the given texts" (p. 23), that is, meaning is not contained in 'the physicality of texts'
                5. instead, "Texts have meanings relative to particular contexts, discourses, or purposes" (p. 24)
                6. CA draw inferences from a body of texts relative to their relative contexts (i.e., relative to the conceptualization of the to-be-inferred phenomenon)
        - *examples* (pp. 26-9)
            - example 5 (p. 28): mass communications
                - Krippendorff criticizes that "[efforts] to describe how something is covered by, 'portrayed' in, or 'represented' in the media invoke a picture theory of content ... [that] hides [researchers'] inferences behind the naive belief that they are able to describe meanings objectively while rendering the results immune to invalidating evidence" (i.e., an example of implicit representationalsim)
            - bottom line: you cannot get rid of interpretation in CA, hence there exists no 'ground truth' meaning of texts 
        - *framework* (pp. 29-)
            - Krippendorff presents his framework for CA with three intentions:
                1. prescription (as a heuristic model for designing CA research)
                2. analytical (as a standard against which CA research can be evaluated)
                3. methodological (as a standard pointing to quality conditions of best practices in CA research)
            - elements
                - (a body of) *texts* (pp. 30f.)
                - a *research question* (pp. 31-3)
                    - a set of hypothesis that can be answered through inferences drawn from texts
                    - four characteristics (pp. 32f.):
                        - answerable (abductively inferable) by examination of a body of texts
                        - delineate a set of possible (i.e., hypothetical) answers
                        - concern currently inaccessible phenomena
                        - allow for validation by acknowledging alternative ways to substantiate the occurrence of the inferred phenomena 
                    - a *context* (pp. 33f.)
                        - "texts acquire significance ... in the context of their use" (p. 33)
                        - contexts are the 'conceptual environment' of a text (p. 33)
                        - in CA, context captures the researcher's assumption about the data generating process by which the text was most likely produced 
                            - [hli:] Krippendorf uses the concept 'context' in somewhat confusing ways
                                - I conceive of it as the conceptualizations of phenomena to be analyzed using CA
                                - this conceptualization must be made explicit for CA to be replicable (p. 24)
                        - context knowledge takes two forms: knowledge of
                            1. the 'network of stable correlations' (NSC) which are assumed to connect texts with to-be-inferred phenomena
                            2. 'contributing conditions' that render assumption about NSC more or less appropriate
                                - [hli:] say anti-elitism is considered a necessary condition of populism, and is argued to be inferable from texts by a set of specific anti-elitist communication strategies (the assumed NSC), then a direct communication channel can be considered a contribution condition for the inference of anti-elitism from texts, for it can be used to express and anti-established-media stance
                        - "once content analysts have chosen the context within which they intend to make sense of a given text, the diversity of interpretations may well be reduced to a manageable number, sometimes to one" (p. 24)
                    - an *analytical construct* (pp. 34-6)
                        - operationalize what n analyst knows about the context
                        - models the texts' context of use
                        - i.e., a measurement instrument
                    - an *inference* (pp. 36-8)
                        - three types of inference:
                            - deductive (inferred from premises, from the general to the particular)
                            - inductive (inferred from examples, from the particular to the general)
                            - abductive (probabilistic inference in one domain based on observations in a logically distinct domain, e.g., from text to abstract concepts)
                        - Krippendorff points out that the tpye of inference involved in CA is abduction
                    - *validating evidence* (pp. 39)
                        - a CA should be validatable in principle
        - *contrasts and comparisons* (pp. 40-43) 
    - **chapter 3: uses and inferences** (pp. 44-)
        - Krippendorf distinguishes six "types of logics capable of relating data to their contexts":
        - *traditional overviews* (pp. 44-7)
        - *extrapolations* (pp. 47-53)
        - *standards* (pp. 54-8)
            - standards are deployed to evaluate observed phenomena to establish 
                1. what kinds they are (identification), 
                2. how good/bad they are (evaluation), or
                3. how close they match expectations (judgement)
        - *indices and symptoms* (pp. 58-)
            - an index is a variable that is associated with other (potentially unobservable) phenomena:
                - examples from mass communications research (p. 59):
                    - *absence/presence* of a reference indicates the source's *awareness/knowledge* of a phenomenon
                    - *frequency* of references indicates the *importance* a source attributes to a phenomenon
                    - *favorable vs. unfavorable references* indicate a source *attitude* towards a phenomenon
                    - *qualifications* added to references indicate the *strength* of a belief, attitude, or motivation
                    - *frequency of co-occurrence* of two references indicates *strength of association* between two phenomena
        - *linguistic representations* (pp. 62-6)
        - *conversations* (pp. 66-8)
        - *institutional processes* (pp. 68-74)
    - **chapter 4: the logic of content analysis** (pp. 81-96)
        - *content analysis design* (pp. 81-)
            - research requires explicitness about methodology
            - research design (RD):= "[the] network of steps a researcher takes to conduct a research project" (p. 81)
            - for a RD to be replicable, a "researcher's descriptive account of the analysis must be complete enough to serve as a set of *instructions* to coders" (p. 82, original emphasis)
            - case selection is also important: "content analysis ... has to address prior question concerning why available texts came into being, what they mean to whom, how they mediate between antecedent and consequent conditions, and, ultimately, whether they enable the analyst to select valid answers to the questions concerning their contexts" (p. 82)
            - *components* of a CA (pp. 83-7)
                1. unitizing
                2. sampling
                3. recording/coding: 
                    - bridges the gap between unitized texts in the sample at hand and coders' readings of them
                    - generate analyzable representations of text
                4. reducing: generating representations of codings
                5. inference: maps measurements back onto conceptual dimensions to arrive at answers to the RQ
                6. narrating answers
            - components 1-4 are part of the data-making of CA
            - *quantitative and qualitative CA* (pp. 87-)
                - "text is always qualitative to begin with, categorizing textual units is considered the most elementary for form of measurement" (p. 87, referring to Stevens, 1946)
                - both (i) "the expliciteness and objectivity of scientific data processing", and (ii) "the appropriateness of the procedures used relative to the chosen context" are indispensable for CA
        - *designs preparatory to CA* (pp. 89-93)
        - *designs exceeding CA* (pp. 93-)
            - comparing similar phenomena inferred from different bodies of text
                - split the data along non-trivial temporal cutoffs (e.g., pre-election periods vs. other periods), or by splitting the data in 
            - testing relationships among phenomena inferred from one body of texts
            - testing hypothesis concerning how content analysis results relate to other variables 
    - **chapter 5: unitizing** (pp. 97-110)
    - **chapter 6: sampling** (pp. 111-24)
    - **chapter 7: recording/coding** (pp. 125-49)
        - *the function of recoding an coding* (pp. 125-7)
            - generate analyzable representations of text from analysts readings of text
            - presupposes the formulation of recording instructions that elicit high levels of reliable measurement
            - *recording*:= the transfer of analysts' readings of a text into analysis/inferences
            - *coding*:= recording based on observer-independent rules
            - recording instructions: 
                - "intended to explicate [coding] rules that minimize the use of subjective judgments in the recording process, without denying the participation of human abilities" (p. 126)
                - need to be replicable, that is, need to explicate
                    1. the qualifications coders need to have,
                    2. the training coders must undergo before beginning with recording and coding,
                    3. the syntax and semantic of the data language (i.e., coding schemes)
                    4. the output format of human codings
            - mechanical measurement instruments (i.e., instruments that formulate explicit and detailed recording instructions and coding rules) are problematic in the analysis of textual data because they deny/do not account for the participation of human abilities
                - [hli:] the analysis of the human abilities expected to be present in a population of potential coders is thus an important prerequisite for formulating good recording instructions (i.e., coding rules that elicit high levels of reliable measurement)
        - *coders qualifications* (pp. 127-9)
            - cognitive abilities (p. 127):
                - coders must be capable of understanding recording instructions and applying them consistently
                    - [hli:] hard to test/evaluate  
            - backgrounds (p. 128)
                - "content analysts should not underestimate the importance of coders' familiarity with the phenomena under consideration"
                - familiarity:= "a sense of understanding that coders must bring to a content analysis"
                - similarity in backgrounds (in terms of education, prior involvement with similar texts, 'social sensitivities') aids reliability by facilitating a similar understanding and application of recording instructions by different coders
            - frequency (pp. 128f.)
                - to ensure replicability, content analysts need to make sure that potential coders are available to other researchers (i.e., the abilities that qualify coders must be common, i.e., they must occur with sufficient frequency)
        - *coder trainings* (pp. 129-31)
            - training is intended to facilitate a shared understanding and application of the recording instructions, and thus increase replicability 
            - the problem of involving coders in conceptual development (p. 130): implicit adjustments may occur (i.e., coders develop a tacit common understanding of what they are expected to code that is not captured in the recording instruction nor explicated in training) that impair replicability
            - Krippendorff argues (p. 131) that 
                1. "the individuals who take part in the development of recording instructions should not be the ones who apply them [for final measurement]"; 
                2. "the finalized instructions should be tested for reliability with a fresh set of coders";
                3. coders reliance on extraneous information when coding "undermines the governance of the recording instructions", impairing reliability; 
                    - [hli:] one may ask what are 'sensitivities' to social and cultural context (considered important coder background qualifications, cf. p. 128) else than 'extraneous information'
                        - it is hard to ensure for the commonality of such sensitivities in coder populations (and perhaps even harder to ensure for their future availability in order to meet a necessary replicability requirement), and it may hence be reasonable to minimize the importance of such qualifications for reliable measurement 
                4. "communication among coders [during coding] challenges the independence of individual coders", again impairing reliability; and
                5. "self-applied recording instructions are notoriously unreliable"
        - *approaches to defining the semantics of data* (pp. 132-42)
            - as a rule of thumb, "the reliability of recording is greatly enhanced if the task that an instruction delineates is natural, relies on familiar conceptual models, and remains close to how the texts to be recorded would be read ordinarily" (p. 132)
            - categories must be mutually exclusive and exhaustive
            - *verbal designations* (pp. 132f.)
                - use labels for categories (verbal designations) that are common and widely understandable (i.e., avoid technical language)
                - however, single-word designations are often inadequate for recording more compley meanings ([hli:] this is Krippendorff's implicit criticism of dictionary-based content analytic methods)
            - *extensional lists* (pp. 133-5)
                - important when conceptions are difficult to communicate
                - enumerating and listing every single category that may be applicable to texts is often cumbersome, however
                - applying extensional list is cumbersome for coders ([hli:] cf. the CMP coding scheme and its demonstrable shortcomings)
            - *decision schemes* (pp. 135f.)
                - each recorded datum (each outcome category) is the outcome of a predefined sequence of decisions
                - allow to organize complex judgments along a sequence of decisions
                - reduces coders' cognitive load
            - *magnitudes and scales* (pp. 136f.)
                - coders are (implicitly) asked to conceive of the to-be-recorded phenomena as continua
                - semantic anchors can be deployed to aid this task
                - however, it cannot always be ensured and hardly controlled for that coders apply the proper continuous conception 
                - also, text is not always scalable (only some phenomena and textual features are continuous)
                - uncodability may be confused with perfect balance
            - *simulation of hypothesis testing* (pp. 137-9)
                - allows to address a text's presuppositions, implications, and omissions (what's written between the lines)
                - coders are asked whether a given text can plausible read as supporting or opposing a stated set of alternative propositions (hypotheses)
                - answers can be recorded on scales allowing for different gradations (e.g., the subjective degree of certainty, or the explicitness with which a given proposition is affirmed/denied)
            - *simulation of interviewing* (pp. 139-41)
                - coders are asked to stand-in for the author of a text in an interview, that is, they are asked to give answers in their place, assuming the author's perspective, opinion, attitude, etc.
                - presupposes very close reading and high and specialized cognitive abilities
            - *constructs for closure* (pp. 141f.)
        - records (pp. )
    - **chapter 8: data languages** (pp. 150-70)
    - **chapter 9: analytical constructs** (pp. 171-)
        - *the role of analytical constructs* (pp. 171-3)
            - the inferential step in CA is of an abductive nature, "because the two domains---texts ... and what theses texts imply ... are logically independent of each other, and bridging this logical gap requires justifications" (p. 171)
            - analytical constructs function as most-probable hypotheses of the function of texts in predefined contexts
                - hence, analytical constructs warrant the intended inference, but need to be backed by knowledge of the context of the analyzed texts (p. 171)
                - put differently, "the abduction is justified by the assumption that the analytical construct is a true or heuristic model of the context"
            - krippendorff formulates a model that relates texts and the targeted inferences:
                - x is the textual data
                - x' is coder's reading/interpretation of x
                - the coder transfers x' into y', her abductive inference from x', using the  analytical construct devised by the researcher
                - y, the actual qualities of the text being analyzed, remains unknown, and inference about it are approximated by y'
                    - hence the relationship between y and y' is intrinsically uncertain
        - *sources of certainty* (pp. 173-9) in developing analytical constructs that (in)validate certain claims about the relationship between construct and context (i.e., between measurement instrument and concept):
            - *previous successes and failures* of other CAs  (pp. 173f.)
            - *expert knowledge and experience* of/with the context (pp. 175f.)
            - *established theories* about context (deductive development of analytical concepts) (pp. 176-9)
            - *embodied practices* sampled from a context (inductive development of analytical concepts) (p. 179)
        - *types of constructs* (pp. 179-85)
            - map onto the different types of logics capable of relating data to their contexts:
                - *extrapolations*
                - *applications of standards*: entails comparison of texts with a given or assumed standard
                - *indices and symptoms*
                    - variables that are (claimed to) correlate with the phenomenon of interest
                    - indicators should not correlate with phenomena that are assumed to be independent from the phenomenon of interest (an indicator must discriminate among phenomena)
                    - indicators should not be affected by variables that are accidental or irrelevant to the phenomenon of interest
                - *re-presentations*:
                    - seen as the purpose of communications
                    - demand discourse specific constructs
                - *conversations/interactions*: tap the interactive meanings  of assertions
                - *institutional processes*
        - *sources of uncertainty* (pp. 185-7)
            - variance of the target
            - confidence levels ("the inductive probability that the analytical construct is not an accidental product of the circumstance of its construction", p. 186)
            - appropriateness of the construct (if the network of correlations relating content and the phenomenon of interest to one another is not stable, or different from what is mapped by the analytical construct)
    - **chapter 10: analytical/representation techniques** (pp. 191-210)
    - **chapter 11: reliability** (pp. 211-)
        - three manifestations of reliability: *stability*, *reproducibility*, and *accuracy*
        - *why reliability?* (pp. 211-)
            - data are the trusted ground for empirical reasoning
            - hence, one wants to be confident that the data
                1. have been generated without (with only minimal) distortions and/or biases
                2. mean the same thing for everyone who uses them
            - generally, reliability asserts that particular research results can be duplicated, and that only negligible amounts of noise have interfered with the measurement process  
            - two conceptions of reliability:
                - measurement theory:
                    - reliability tests seek to establish that the data produced are independent of the measuring event, instrument, or person (p. 211, citing Kaplan and Goldsen, 1965, 83f.) 
                    - "a *research procedure is reliable* when it responds to the same phenomena in the same way regardless of the circumstances of its implementation" (p. 211, emphasis added)
                - interpretivist:
                    - acknowledges that in CA, the phenomena of interest and its expression in texts  is elusive
                    - "reliability is the degree to which members of a designated community agree on the readings, interpretations, responses to, or uses of given texts or data" (p. 212)
            - in either case, researcher need to demonstrate the trustworthiness of their data
                - this *always* requires data in addition to that generated by the CA whichs reliability is sought to be established, so-called *reliability data* (p. 212)
            - reliability is generally not concerned with facts extraneous to the measurement process (in contrast to validity)
            - relationship between validity and reliability (pp. 212-4):  "reliability is a necessary, but not a sufficient, condition for validity" (p. 213)
                1. unreliability limits the chances of validity (unreliable measurements will scatter more strongly, so it is more likely that a single measurement is off-target)
                2. reliability does not guarantee validity (even repeated measurements that scatter only minimally may be consistently off-target)
        - *reliability designs* (pp. 214-21)
            - *types of reliability* (pp. 214-6)
                - differ in what constitutes the reliability data
                - *stability*
                    - design: test-retest
                    - sources of disagreement: intra-coder inconsistencies
                    - reliability data: repeated measurements by the same coders
                    - "too weak to serve as a reliability measure in CA", for it cannot rule out that consistent measurement results from individual coder effects rather than the reliability of the measurement process
                - *reproducibility* (inter-coder reliability)
                    - design: test-test
                    - sources of disagreement: intra-coder inconsistencies AND inter-coder disagreement
                    - reliability data: measurements obtained by other coders, by applying a different (functionally equivalent) measurement instrument, or different conditions  
                - *accuracy* (coding/annotation quality)
                    - design: test-standard
                    - sources of disagreement: intra-coder inconsistencies AND inter-coder disagreement AND deviations from a standard
                    - reliability data: an external ('gold') standard (e.g., established by a panel of experts)
                        - however, this involves "the privileging of some interpretations over others, and thus puts any claims regarding precision or accuracy on epistemologically shaky grounds" (p. 216)
                    - defined as "the degree to which a [measurement] process conforms to its specifications and yields what it is designed to yield" (p. 215)
                    - strongest possible reliability test
            - *conditions for generating reliability data* (pp. 216-9)
                - to use inter-coder agreement as an indicator of reproducibility, the following three aspects of the measurement process must be communicable (i.e., made explicit and well-documented):
                    1. coding instructions
                    2. criteria for selecting  coders
                    3. the independent working of coders
                - Krippendorff argues against
                    - allowing coders to discuss their interpretations in order to achieve majority vote or consensus codings, due to the potential impact of social dynamics and effects of status differentials on coding decisions (p. 217)
                    - evaluating on the same set of coders that was involved in the refinement of the recording instructions 
                    - selecting experts for coding, for there may not exist other coders with matching qualifications, which would, in turn, prevent from assessing their agreement, and hence impair reproducibility
                - instead, a two-step approach should be adopted to make reliability assessable and the data generated more trustworthy
                    1. employ 3+ coders to independently code the data, and measure intercoder reliability
                    2. use a decision rule to arbitrate disagreements (majority judgment, averaging, or post-coding consensus finding)
                        - however, "the only publishable reliability is the one measured before the reconciliation of disagreement" (p. 219)
        - *$\alpha$-agreement for coding* (pp. 221-)




