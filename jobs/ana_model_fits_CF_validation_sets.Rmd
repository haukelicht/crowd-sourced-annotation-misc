---
title: "Analyse annotation models fitted to crowd-sourced validation dataset on populist instances among political statements on Twitter and Facebook."
author: "Hauke Licht"
date: "2019-02-19"
output: html_document
---
  
```{r knitr_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
cat(getwd())
```

## Setup
```{r setup}  
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
library(viridisLite)
theme_set(theme_minimal())

plot_path <- "./plots"
```


```{r helpers}
save_plot <- partial(ggsave, path = plot_path, device = "png", width = 7, height = 7)

get_bs_sample_prevalences <- function(bs.fits, keep.categories){
  map_df(bs.fits, "est_class_prevl", .id = "boot_sample") %>% 
    gather("meth", "prob", est_prob:prop_maj_winner, -category) %>% 
    mutate(
      boot_sample = as.integer(boot_sample)
      , meth = case_when(
        grepl("^est_", meth) ~ "model-based classification (MC)", 
        grepl("winner$", meth) ~ "majority voting (MV)",
        TRUE ~ "proportion of messages"
      )
    ) %>% 
    filter(category %in% keep.categories, !grepl("^prop", meth)) %>% 
    select(boot_sample, category, meth, prob) %>% 
    arrange(boot_sample, category, meth) %>% 
    tbl_df()
}
```


## Read in Data

```{r read_data}
fits <- readRDS("./../exdata/em_fits_CF_validation_set.RData")
str(fits, 1)
```

The object `fits` is a list object.

### The original annotations data

Element `data` contains the original annotation data.
This data was collected as part of the validation of the crowd-sourced measruements of populism in social media posts.
Data generation involved recruiting crowd workers in *CrowdFlower* (CF), who were asked to judgement post along several measurement dimensions, three of which are of central interests here:

- *anti-elitism*: coders were asked to judge whether the post criticizes the elites (binary indicator in column `elite`)
- *people-centrisum*: coders were asked to judge whether the post praises the people and/or ordinary citizens  (binary indicator in column `people`)
- *exclusionism*: coders were asked to judge whether the post explicitly excluded certain groups from 'the people' and/or 'ordinary' citizens (binary indicator in column `exclusionary`)
  
Each post was coded by between one to three coders.

```{r examine_annotations_data}
fits$data %>% 
  group_by(item) %>% 
  summarize(judgements = n_distinct(annotator)) %>% 
  group_by(judgements) %>% 
  summarize(n = n())
```

### Fitted annotation models and bootstraps

The annotation data has been used to classify items into classes along the three measurement dimensions.
Specifically, Dawid and Skene's original annotation model has been fitted using the expectation-maximization algorithm ([Dawid and Skene, 1979](https://www.jstor.org/stable/2346806)).
This model estimates three sets of parameters:

- *class prevalences* $\rho_k\ \forall\ k \in \mathcal{K}$, the set of outcome categories/classes (corpus-level)
- *class membership probabilities* $\pi_{ik}$ (item-level)
- *annotator sensitivities* and *specificities*

The elements `anti_elitism`, `people_centrism`, and `exclusionary`, respectively, of the object `fits` contain each two elements:

- `fit`, a list object containing the parameter estimates obtained by fitting the Dawid-Skene model to the respective annotations data, and 
- `bootstraps`, a list object containing fitted objects obtained by fitting the Dawid-Skene model to each of 500 bootstrap samples generated by resampling judgements at the item-level (i.e., all judgements of 167 randomly sampled items) from the original data the respective annotations data.
  
Below, this data will be used below to examine annotation quality.

## Examining annotation quality

### Anti-elitism

```{r get_ae_fits}
ae_fits <- fits$anti_elitism
```

##### Prevalence 

```{r inspect_ae_prevalence_estimate}
ae_fits$fit$est_class_prevl
```
The prevalence estimate obtained by aggregating judgements at the item-level by simple majority-vote is `r ae_fits$fit$est_class_prevl[2, 4]*100` % and is larger than 
the parametric prevalence estimate of anti-elite statements (
`r ae_fits$fit$est_class_prevl[2, 2]*100` %) obtained
from the annotation model.

If we determine items' classes based on estimated class membership and compute the porportion of items classified as anti-elite statements, we get
```{r compute_ae_prevalence}
ae_fits$fit$est_class_probs %>% 
  select(-majority_vote) %>% 
  spread(category, est_prob, sep = "_") %>% 
  mutate(is_antielite = category_TRUE > category_FALSE) %>% 
  summarize(prevalence = sum(is_antielite)/n())
```

If we aggregate the 500 bootstrapped model fits, we can see that majority-voting systematically yields higher prevalence estimates as compared to parametric estimation. 
```{r plot_ae_prevalence, fig.align='center', fig.height=5, fig.width=7, echo=FALSE}
ae_sampl_prevalences <- get_bs_sample_prevalences(ae_fits$bootstraps, c(TRUE))

t_diff <- t.test(data = ae_sampl_prevalences, prob ~ meth, alternative= "greater")

p_ae_prevalence_voted_v_est <- ggplot(
  data = ae_sampl_prevalences,
  aes(x = prob, fill = meth)
) + 
  geom_density(color = NA) +
  scale_fill_viridis_d(alpha = .5) + 
  labs(
    title = "Estimated prevalence of anti-elite statements",
    subtitle = "Based on 500 bootstrap samples of (multiply) annotated statements",
    x = "Est. prevalence",
    y = "",
    fill = "Decision rule"
  ) +
  geom_vline(xintercept = t_diff$estimate) + 
  annotate(geom = "text"
           , x = t_diff$estimate[1] + .001 , y = 2
           , label = paste("hat(italic(mu))[MV] ==", round(t_diff$estimate[1], 3))
           , parse = TRUE
           , hjust = 0
  ) +
  annotate(geom = "text"
           , x = t_diff$estimate[2] + .001 , y = 2
           , label = paste("hat(italic(mu))[MC] ==", round(t_diff$estimate[2], 3))
           , parse = TRUE
           , hjust = 0
  ) +
  labs(
    caption=bquote(
      "One-tailed"~
        italic(t)*"-test"~of~H[0]*": "~
        hat(italic(mu))[MV] <= hat(italic(mu))[MC]~
        "gives"~italic(t) %~~% .(round(t_diff$statistic, 3))*
        ", "~
        italic(p) %~~% .(round(t_diff$p.value, 5))
    )
  ) +
  theme(
    legend.position = "top"
    , plot.caption = element_text(hjust = 0)
  )


# distributions of bootstrapped point estimates tightly overlap
p_ae_prevalence_voted_v_est
# We see that 
save_plot(filename = "ae_voted_v_est_prevalence.png", plot = p_ae_prevalence_voted_v_est) 
```


#### Classification (un)certainty 

In order to quantify uncertainty in model-based classifications,
I obtained the probabilistic model-based class membership point estimates from 500 bootsrapped model fits and summaize them at the item-level.
This allows to compute item-level summary statistics, and to inspect the distribution of parameter estimates and its moments.
Specifically, the item-level variation in class membership probabilites $\pi_{i1}$ can be visualized in a violin plot that I overlayed with point estimates (means of bootsrapped samples) and error bars (mean Â± one standard-deviation).
I then used colors to distinguish between different types of judgements sets and their behavior under majority voting:

- *pure negative* (*pure positive*) judgements sets are those consisting of only negative (positive) judgements, and on which majority-voting induces negative classification
- *impure negative* (*impure positive*) judgements sets are those where less than 50% but more than 0% of judgements are positive (negative), and on which majority-voting induces negative (positive) classification
- *tie* judgement sets are those with exactly 50:50 positive:negative judgements, and on which majority-voting induces no classification without specifiying a tie-breaking rule

```{r plot_ae_classification_uncertainty, fig.align='center', fig.height=7, fig.width=7, echo=FALSE}

ae_classification_uncertainty_data <- 
  # get judgement set sizes from annotation data
  fits$data %>% 
    group_by(item, text)  %>% 
    summarize(
      n = n()
      , n_elites = sum(elites)
    ) %>% 
    # left-join bootstrapped class probability estimates
    left_join(
      map_df(ae_fits$bootstraps, "est_class_probs", .id = "boot_sample") %>% 
        filter(category) %>% 
        group_by(item) %>% 
        mutate(
          est_prob_sd = sd(est_prob)
          , prop_mv = sum(majority_vote)/n()
        ) %>% 
        arrange(item)
    ) %>% 
    mutate(
      voted = n_elites/n
      , voted = case_when(
          voted == 1 ~ "pure positive",
          voted > .5 ~ "impure positive",
          voted == .5 ~ "tie",
          voted == 0 ~ "pure negative",
          voted < .5 ~ "impure negative",
          TRUE ~ NA_character_
      )
    )

p_ae_classification_uncertainty_model_vs_voting <- ggplot(
  data = ae_classification_uncertainty_data %>% 
    # keep only items characterized by substantial measurement uncertainty
    filter(est_prob_sd > .01),
  mapping = aes(x = reorder(factor(item), est_prob), y = est_prob, fill = voted)
) + 
  geom_violin(
    scale = "width"
    , adjust = .75
    , colour = NA
    , width = .75
  ) + 
  stat_summary(
    aes(group = factor(item))
    , fun.y = mean
    , geom = "point"
    , fill = "black"
    , shape = 1, size = 1
    , position = position_dodge(width = .7)
  ) + 
  stat_summary(
    fun.data = function(x, n = 1) {
      tibble(
        y = mean(x),
        sd = sd(x),
        ymin = y - n*sd,
        ymax = y + n*sd)
    }
    , geom = "errorbar"
    , colour = "black"
    , width = .25
    , size = .25
    , position = position_dodge(width = .7)
  ) + 
  scale_fill_viridis_d(alpha = .75) +
  coord_flip() +
  labs(
    title = "Anti-elite statements: Uncertainty in model- vs. voting-based classification",
    # subtitle = sprintf("Uncertainty in model-based classification computed based on 500 bootstrap samples of %s statements each", ssize),
    y = "Est. probability of being an anti-elite statement (model-based)",
    x = "", # "Message ID",
    fill = "Judgement sets:",
    caption = paste(
      ""
      , "Model-based classifications obtained by fitting Dawid-Skene annotation model to each of 500 resampled judgement sets."
      , "Uncertainty in model-based classification displayed as the variability in estimated class-membership probabities across reasampled jugement sets"
      , "Majority-vote classifications are generated by aggregating judgement sets at the level of text."
      , "Statments with a standard deviation in est. class probabilities > .01 not displayed."
      , sep = "\n"
    )
  ) + 
  theme(
    legend.position = "top"
    , plot.caption = element_text(hjust = 0)
  )

p_ae_classification_uncertainty_model_vs_voting
save_plot(
  filename = "p_ae_classification_uncertainty_model_vs_voting.png", 
  plot = p_ae_classification_uncertainty_model_vs_voting) 
```

When looking only at items where class probabilities $\pi_{i1}$ vary substantively across boottrapped model fits (i.e., $\mbox{SD}(\pi_{i1}) \geq 0.01$),
two things become apparent:

1. When using the original annotation data to classify items along the anti-elitism dimension by means of majority-voting, the measurement uncertainty revealed by bootsrapping annotation model fits is obscured: While items with only negative judgements are characterized by relatively low model-based classification uncertainty, this does not hold for 'impure negative' judgements sets. The latter applies to 'impure positive' items, too. Majority-voting based aggregation thus fails to convey the substantial measurement uncertainty that characterizes some items.
2. Where judgements tie, model-based classification allows to disambiguate in the overwhelming number of resampled fits: Some of the items with tied judgements are estimated to belong to the class of anti-elite statements in most bootstrapped samples, whereas for other items in this group model-based classification overwhelmingly assigns them to the non-anti-elite statement class. Thus, where simple majority-voting fails to disambiguate (and hence a random tie-breaker would need to be consulted), model-based classification is a reliable method to achieve this goal consistently.
  
<!--```{r}
ae_classification_uncertainty_data %>% 
  filter(est_prob_sd > .01) %>% 
  group_by(voted, item, text) %>% 
  summarize(
    est_class1 = mean(est_prob) > .5
    , est_prob_avg = mean(est_prob)
    , est_prob_sd = mean(est_prob_sd)
  ) %>% 
  group_by(voted, est_class1) %>% 
  sample_n(1) %>% 
  select(voted, est_class1, item, est_prob_avg, est_prob_sd, text) %>% 
  knitr::kable(
    digits = 3
    , col.names = c("Judgement set", "Anti-elite?", "Item-ID", "$\\bar{\\pi}_{i1}$", "$\\mbox{SD}(\\pi_{i1})$", "Statement text"))
```-->





### People-centrism

```{r get_pc_fits}
pc_fits <- fits$people_centrism
```

##### Prevalence 

```{r inspect_pc_prevalence_estimate}
pc_fits$fit$est_class_prevl
```

The prevalence estimate obtained by aggregating judgements at the item-level by simple majority-vote is `r pc_fits$fit$est_class_prevl[2, 4]*100` % and is larger than 
the parametric prevalence estimate of anti-elite statements (
`r pc_fits$fit$est_class_prevl[2, 2]*100` %) obtained
from the annotation model.

If we determine items' classes based on estimated class membership and compute the porportion of items classified as anti-elite statements, we get
```{r compute_pc_prevalence}
pc_fits$fit$est_class_probs %>% 
  select(-majority_vote) %>% 
  spread(category, est_prob, sep = "_") %>% 
  mutate(is_pc = category_TRUE > category_FALSE) %>% 
  summarize(prevalence = sum(is_pc)/n())
```

If we aggregate the 500 bootstrapped model fits, we can see that majority-voting systematically yields higher prevalence estimates as compared to parametric estimation. 
A caveat of the model-based aggregation of this particular data is that the estimated prevalence of people-centrism statements is highly dependent on the judgement sets used to fit the annotation model, as indicated by the multimodality and wide spread of the density of the model-based prevalence estimates.

```{r plot_pc_prevalence, fig.align='center', fig.height=5, fig.width=7, echo=FALSE}
pc_sampl_prevalences <- get_bs_sample_prevalences(pc_fits$bootstraps, c(TRUE))

t_diff <- t.test(data = pc_sampl_prevalences, prob ~ meth, alternative= "greater")

p_pc_prevalence_voted_v_est <- ggplot(
  data = pc_sampl_prevalences,
  aes(x = prob, fill = meth)
) + 
  geom_density(color = NA) +
  scale_fill_viridis_d(alpha = .5) + 
  labs(
    title = "Estimated prevalence of people-centrism statements",
    subtitle = "Based on 500 bootstrap samples of (multiply) annotated statements",
    x = "Est. prevalence",
    y = "",
    fill = "Decision rule"
  ) +
  geom_vline(xintercept = t_diff$estimate) + 
  annotate(geom = "text"
           , x = t_diff$estimate[1] + .001 , y = 2
           , label = paste("hat(italic(mu))[MV] ==", round(t_diff$estimate[1], 3))
           , parse = TRUE
           , hjust = 0
  ) +
  annotate(geom = "text"
           , x = t_diff$estimate[2] + .001 , y = 2
           , label = paste("hat(italic(mu))[MC] ==", round(t_diff$estimate[2], 3))
           , parse = TRUE
           , hjust = 0
  ) +
  labs(
    caption=bquote(
      "One-tailed"~
        italic(t)*"-test"~of~H[0]*": "~
        hat(italic(mu))[MV] <= hat(italic(mu))[MC]~
        "gives"~italic(t) %~~% .(round(t_diff$statistic, 3))*
        ", "~
        italic(p) %~~% .(round(t_diff$p.value, 5))
    )
  ) +
  theme(
    legend.position = "top"
    , plot.caption = element_text(hjust = 0)
  )


# distributions of bootstrapped point estimates tightly overlap
p_pc_prevalence_voted_v_est
# We see that 
save_plot(filename = "pc_voted_v_est_prevalence.png", plot = p_pc_prevalence_voted_v_est) 
```


#### Classification (un)certainty 

In order to quantify uncertainty in model-based classifications of people-centrism statements, I replicated the procedure described above to compute item-level summary statistics, and to inspect the distribution of parameter estimates and its moments.

```{r plot_pc_classification_uncertainty, fig.align='center', fig.height=7, fig.width=7, echo=FALSE}

pc_classification_uncertainty_data <- 
  # get judgement set sizes from annotation data
  fits$data %>% 
    group_by(item, text)  %>% 
    summarize(
      n = n()
      , n_people = sum(people)
    ) %>% 
    # left-join bootstrapped class probability estimates
    left_join(
      map_df(pc_fits$bootstraps, "est_class_probs", .id = "boot_sample") %>% 
        filter(category) %>% 
        group_by(item) %>% 
        mutate(
          est_prob_sd = sd(est_prob)
          , prop_mv = sum(majority_vote)/n()
        ) %>% 
        arrange(item)
    ) %>% 
    mutate(
      voted = n_people/n
      , voted = case_when(
        voted == 1 ~ "pure positive",
        voted > .5 ~ "impure positive",
        voted == .5 ~ "tie",
        voted == 0 ~ "pure negative",
        voted < .5 ~ "impure negative",
        TRUE ~ NA_character_
      )
    )

p_pc_classification_uncertainty_model_vs_voting <- ggplot(
  data = pc_classification_uncertainty_data %>% 
    # keep only items characterized by substantial measurement uncertainty
    filter(est_prob_sd > .02),
  mapping = aes(x = reorder(factor(item), est_prob), y = est_prob, fill = voted)
) + 
  geom_violin(
    scale = "width"
    , adjust = .75
    , colour = NA
    , width = .75
  ) + 
  stat_summary(
    aes(group = factor(item))
    , fun.y = mean
    , geom = "point"
    , fill = "black"
    , shape = 1, size = 1
    , position = position_dodge(width = .7)
  ) + 
  stat_summary(
    fun.data = function(x, n = 1) {
      tibble(
        y = mean(x),
        sd = sd(x),
        ymin = y - n*sd,
        ymax = y + n*sd)
    }
    , geom = "errorbar"
    , colour = "black"
    , width = .25
    , size = .25
    , position = position_dodge(width = .7)
  ) + 
  scale_fill_viridis_d(alpha = .75) +
  coord_flip() +
  labs(
    title = "People-centrism statements: Uncertainty in model- vs. voting-based classification",
    # subtitle = sprintf("Uncertainty in model-based classification computed based on 500 bootstrap samples of %s statements each", ssize),
    y = "Est. probability of being an people-centrism statement (model-based)",
    x = "", # "Message ID",
    fill = "Judgement sets:",
    caption = paste(
      ""
      , "Model-based classifications obtained by fitting Dawid-Skene annotation model to each of 500 resampled judgement sets."
      , "Uncertainty in model-based classification displayed as the variability in estimated class-membership probabities across reasampled jugement sets"
      , "Majority-vote classifications are generated by aggregating judgement sets at the level of text."
      , "Statments with a standard deviation in est. class probabilities > .02 not displayed."
      , sep = "\n"
    )
  ) + 
  theme(
    legend.position = "top"
    , plot.caption = element_text(hjust = 0)
  )

p_pc_classification_uncertainty_model_vs_voting
save_plot(
  filename = "p_pc_classification_uncertainty_model_vs_voting.png", 
  plot = p_pc_classification_uncertainty_model_vs_voting) 
```

When looking only at items where class probabilities $\pi_{i1}$ vary substantively across boottrapped model fits (i.e., $\mbox{SD}(\pi_{i1}) \geq 0.02$),
two things become apparent:

1. When using the original annotation data to classify items along the anti-elitism dimension by means of majority-voting, the measurement uncertainty revealed by bootsrapping annotation model fits is obscured, but classifications largely correspond.
2. Where judgements tie, model-based classification allows to disambiguate in virtually all resampled fits, and hence again: model-based classification is a reliable method to achieve this goal consistently where simple majority-voting fails to disambiguate (and hence a random tie-breaker would need to be consulted).


### Exclusionism

```{r get_ex_fits}
ex_fits <- fits$exclusionary
```

##### Prevalence 

```{r inspect_ex_prevalence_estimate}
ex_fits$fit$est_class_prevl
```

The prevalence estimate obtained by aggregating judgements at the item-level by simple majority-vote is `r ex_fits$fit$est_class_prevl[2, 4]*100` % and is close to 
the parametric prevalence estimate of exclusionary statements (
`r ex_fits$fit$est_class_prevl[2, 2]*100` %) obtained
from the annotation model.

If we determine items' classes based on estimated class membership and compute the porportion of items classified as exclusionary statements, we get
```{r compute_ex_prevalence}
ex_fits$fit$est_class_probs %>% 
  select(-majority_vote) %>% 
  spread(category, est_prob, sep = "_") %>% 
  mutate(is_pc = category_TRUE > category_FALSE) %>% 
  summarize(prevalence = sum(is_pc)/n())
```

If we aggregate the 500 bootstrapped model fits, we can see that majority-voting systematically yields higher prevalence estimates as compared to parametric estimation. 
A caveat of the model-based aggregation of this particular data is that the estimated prevalence of people-centrism statements is highly dependent on the judgement sets used to fit the annotation model, as indicated by the multimodality and wide spread of the density of the model-based prevalence estimates.

```{r plot_ex_prevalence, fig.align='center', fig.height=5, fig.width=7, echo=FALSE}
ex_sampl_prevalences <- get_bs_sample_prevalences(ex_fits$bootstraps, c(TRUE))

t_diff <- t.test(data = ex_sampl_prevalences, prob ~ meth, alternative= "greater")

p_ex_prevalence_voted_v_est <- ggplot(
  data = ex_sampl_prevalences,
  aes(x = prob, fill = meth)
) + 
  geom_density(color = NA) +
  scale_fill_viridis_d(alpha = .5) + 
  labs(
    title = "Estimated prevalence of exclusionary statements",
    subtitle = "Based on 500 bootstrap samples of (multiply) annotated statements",
    x = "Est. prevalence",
    y = "",
    fill = "Decision rule"
  ) +
  geom_vline(xintercept = t_diff$estimate) + 
  annotate(geom = "text"
           , x = t_diff$estimate[1] + .001 , y = 2
           , label = paste("hat(italic(mu))[MV] ==", round(t_diff$estimate[1], 3))
           , parse = TRUE
           , hjust = 0
  ) +
  annotate(geom = "text"
           , x = t_diff$estimate[2] + .001 , y = 2
           , label = paste("hat(italic(mu))[MC] ==", round(t_diff$estimate[2], 3))
           , parse = TRUE
           , hjust = 0
  ) +
  labs(
    caption=bquote(
      "One-tailed"~
        italic(t)*"-test"~of~H[0]*": "~
        hat(italic(mu))[MV] <= hat(italic(mu))[MC]~
        "gives"~italic(t) %~~% .(round(t_diff$statistic, 3))*
        ", "~
        italic(p) %~~% .(round(t_diff$p.value, 5))
    )
  ) +
  theme(
    legend.position = "top"
    , plot.caption = element_text(hjust = 0)
  )


p_ex_prevalence_voted_v_est

save_plot(filename = "ex_voted_v_est_prevalence.png", plot = p_ex_prevalence_voted_v_est) 
```


#### Classification (un)certainty 

In order to quantify uncertainty in model-based classifications of people-centrism statements, I replicated the procedure described above to compute item-level summary statistics, and to inspect the distribution of parameter estimates and its moments.

```{r plot_ex_classification_uncertainty, fig.align='center', fig.height=7, fig.width=7, echo=FALSE}

ex_classification_uncertainty_data <- 
  # get judgement set sizes from annotation data
  fits$data %>% 
    group_by(item, text)  %>% 
    summarize(
      n = n()
      , n_exclusionary = sum(exclusionary)
    ) %>% 
    # left-join bootstrapped class probability estimates
    left_join(
      map_df(ex_fits$bootstraps, "est_class_probs", .id = "boot_sample") %>% 
        filter(category) %>% 
        group_by(item) %>% 
        mutate(
          est_prob_sd = sd(est_prob)
          , prop_mv = sum(majority_vote)/n()
        ) %>% 
        arrange(item)
    ) %>% 
    mutate(
      voted = n_exclusionary/n
      , voted = case_when(
        voted == 1 ~ "pure positive",
        voted > .5 ~ "impure positive",
        voted == .5 ~ "tie",
        voted == 0 ~ "pure negative",
        voted < .5 ~ "impure negative",
        TRUE ~ NA_character_
      )
    )

p_ex_classification_uncertainty_model_vs_voting <- ggplot(
  data = ex_classification_uncertainty_data %>% 
    # keep only items characterized by substantial measurement uncertainty
    filter(est_prob_sd > .01),
  mapping = aes(x = reorder(factor(item), est_prob), y = est_prob, fill = voted)
) + 
  geom_violin(
    scale = "width"
    , adjust = .75
    , colour = NA
    , width = .75
  ) + 
  stat_summary(
    aes(group = factor(item))
    , fun.y = mean
    , geom = "point"
    , fill = "black"
    , shape = 1, size = 1
    , position = position_dodge(width = .7)
  ) + 
  stat_summary(
    fun.data = function(x, n = 1) {
      tibble(
        y = mean(x),
        sd = sd(x),
        ymin = y - n*sd,
        ymax = y + n*sd)
    }
    , geom = "errorbar"
    , colour = "black"
    , width = .25
    , size = .25
    , position = position_dodge(width = .7)
  ) + 
  scale_fill_viridis_d(alpha = .75) +
  coord_flip() +
  labs(
    title = "Exclusionary statements: Uncertainty in model- vs. voting-based classification",
    # subtitle = sprintf("Uncertainty in model-based classification computed based on 500 bootstrap samples of %s statements each", ssize),
    y = "Est. probability of being a exclusionary statement (model-based)",
    x = "", # "Message ID",
    fill = "Judgement sets:",
    caption = paste(
      ""
      , "Model-based classifications obtained by fitting Dawid-Skene annotation model to each of 500 resampled judgement sets."
      , "Uncertainty in model-based classification displayed as the variability in estimated class-membership probabities across reasampled jugement sets"
      , "Majority-vote classifications are generated by aggregating judgement sets at the level of text."
      , "Statments with a standard deviation in est. class probabilities > .01 not displayed."
      , sep = "\n"
    )
  ) + 
  theme(
    legend.position = "top"
    , plot.caption = element_text(hjust = 0)
  )

p_ex_classification_uncertainty_model_vs_voting
save_plot(
  filename = "p_ex_classification_uncertainty_model_vs_voting.png", 
  plot = p_ex_classification_uncertainty_model_vs_voting) 
```

When looking only at items where class probabilities $\pi_{i1}$ vary substantively across boottrapped model fits (i.e., $\mbox{SD}(\pi_{i1}) \geq 0.01$),
two things become apparent:

1. When using the original annotation data to classify items along the exclusionary dimension by means of majority-voting, the measurement uncertainty revealed by bootsrapping annotation model fits is generally obscured. But the number of items where measurement uncertainty come close the .5 classification cutoff is substantially negligible. 
2. Where judgements tie, model-based classification allows to disambiguate in all resampled fits.

  
  
## Examining coder qualities

Lets examine the sensititvities (true-positive rates) and specificities (true-negative rates) of our three coders
```{r}

anno_perf <- fits[2:4] %>% 
  map_df(.id = "indicator", .f = function(x) {
    map_df(x$bootstraps, "est_annotator_params", .id = "boot_sample")
  }) %>% 
  mutate(
    metric = case_when(
      category & labeled ~ "true-positive rate (sensitivity)",
      !category & labeled ~ "false-positive rate (1 - sensitivity)",
      category & !labeled ~ "false-negative rate (1 - specificity)",
      !category & !labeled ~ "true-negative rate (specificity)"
    )
  ) %>% 
  select(indicator, boot_sample, annotator, metric, est_prob)

p_annotator_biases <- ggplot(
  data = anno_perf %>% 
    filter(grepl("^true", metric)) %>% 
    # filter(indicator == "anti_elitism") %>% 
    arrange(desc(metric))
  , aes(x = factor(annotator), 
        y = est_prob,
        # group = factor(indicator), 
        fill = factor(annotator),
        color = factor(annotator)
    )
) +
  # geom_boxplot(
  #   notch = TRUE,
  #   notchwidth = 1,
  #   outlier.shape = NA,
  #   show.legend = FALSE
  # ) +
  geom_violin(scale = "width") +
  scale_fill_viridis_d(alpha = .5) + 
  scale_color_viridis_d(alpha = .5, guide = FALSE) + 
  facet_grid(
    cols = vars(metric)
    , rows = vars(indicator)
    # rows = vars(metric)
    # , cols = vars(indicator)
  ) +
  labs(
    title = "Estimated true-positive and true-negative rates of annotators",
    subtitle = "in 500 bootstrap samples of %s statements each",
    y = "",
    x = "Annotator ID",
    fill = "Annotator"
  )

p_annotator_biases
save_plot(filename = "annotator_biases.png", plot = p_annotator_biases)
```


```{r}
anno_perf %>% 
  # filter(indicator == "anti_elitism") %>% 
  filter(grepl("\\(specificity\\)$", metric)) %>% 
  group_by(indicator, annotator) %>% 
  summarize(
    avg = mean(est_prob)
    , sd = sd(est_prob)
    , n_na = sum(is.na(est_prob))
  )
```
There are several things worht mentioning:

1. Apparantly, the bootstrapping procedure used here partially fails to induce variation in annotator sensitivity and specificity estimates. This is the case for 
    - estimating annotator 1's specificity on all three measrument dimensions, and her sensitivity for the people-cnetrism classification,
    - estimating annotator 2's specificity for exclusionary and people-centrism classification, and her sensitivity for anti-elitism classification, as well as
    - estimating annotator 3's sensitivity for anti-elitism and exclusionary classification.
2. Generally, sensitivity estimates for people-centrism classification are unstable (i.e., multmodal and characterized by wide spread) for annotators 1 and 2.
3. With regard to the remaining estimates we see that both specificities and sensitivities are fairly high with values most bootstrapped estimates ranging in $[.6, .95]$
4. Only annotator 1's positive classifications on the anti-elite dimension tend to be negatively correlated with model-based classifications. While this puts into question her quality as a coder of anti-elitism in statements, this is an important discovery, since this fact would not have come to light when using simple majority voting as an aggregation method.

